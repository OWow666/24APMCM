{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "我们采用基于Unet和注意力机制的扩散模型并使用EUVP数据集来进行水下图像模型的训练。\n",
    "EUVP数据集（Enhance Underwater Visual Perception Dataset）是一个专门用于水下图像处理的公开数据集，主要用于研究和开发水下图像增强和恢复技术。"
   ],
   "id": "57eba152d37b1dad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sympy.printing.cxx import reserved\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "根据EUVP准备所需数据集",
   "id": "b42329c4a1d0f52f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), 'EUVP', 'Paired', 'underwater_dark')\n",
    "imgs_path = os.path.join(dataset_path, 'trainA')\n",
    "clear_imgs_path = os.path.join(dataset_path, 'trainB')\n",
    "\n",
    "class UnderWaterDataset(Dataset):\n",
    "    def __init__(self, origin_path, res_path, transform=None):\n",
    "        self.origin_path = origin_path\n",
    "        self.res_path = res_path\n",
    "        self.transform = transform\n",
    "        self.origin_imgs = os.listdir(self.origin_path)\n",
    "        self.res_imgs = os.listdir(self.res_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.origin_imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        origin_img_path = os.path.join(self.origin_path, self.origin_imgs[idx])\n",
    "        res_img_path = os.path.join(self.res_path, self.res_imgs[idx])\n",
    "        origin_img = Image.open(origin_img_path).convert('RGB')\n",
    "        res_img = Image.open(res_img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            origin_img = self.transform(origin_img)\n",
    "            res_img = self.transform(res_img)\n",
    "        return origin_img, res_img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()])\n",
    "train_dataset = UnderWaterDataset(imgs_path, clear_imgs_path, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)"
   ],
   "id": "880cfdf15b46b066"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "架构Diffusion Model和Unet",
   "id": "79671c0a9b6c8d51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, base_channels=64):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.enc1 = self.conv_block(in_channels, base_channels)\n",
    "        self.enc2 = self.conv_block(base_channels, base_channels * 2)\n",
    "        self.enc3 = self.conv_block(base_channels * 2, base_channels * 4)\n",
    "        \n",
    "        self.bottleneck = self.conv_block(base_channels * 4, base_channels * 8)\n",
    "        \n",
    "        self.dec3 = self.conv_block(base_channels * 8, base_channels * 4)\n",
    "        self.dec2 = self.conv_block(base_channels * 4, base_channels * 2)\n",
    "        self.dec1 = self.conv_block(base_channels * 2, base_channels)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.ConvTranspose2d(base_channels * 4, base_channels * 4, kernel_size=2, stride=2)\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        \n",
    "        bottleneck = self.bottleneck(self.pool(enc3))\n",
    "        \n",
    "        dec3 = self.upsample(bottleneck) + enc3\n",
    "        dec3 = self.dec3(dec3)\n",
    "        dec2 = self.upsample(dec3) + enc2\n",
    "        dec2 = self.dec2(dec2)\n",
    "        dec1 = self.upsample(dec2) + enc1\n",
    "        dec1 = self.dec1(dec1)\n",
    "        \n",
    "        return dec1\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class DiffusionModel:\n",
    "    def __init__(self, model, img_size, timesteps, device):\n",
    "        self.model = model\n",
    "        self.img_size = img_size\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        self.beta = torch.linspace(0.0001, 0.02, timesteps).to(device)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_cumprod = torch.cumprod(self.alpha, dim=0)\n",
    "        self.alpha_cumprod_prev = F.pad(self.alpha_cumprod[:-1], (1, 0), value=1.0)\n",
    "    \n",
    "    def forward_diffusion(self, x0, t):\n",
    "        noise = torch.randn_like(x0).to(self.device)\n",
    "        sqrt_alpha_cumprod_t = torch.sqrt(self.alpha_cumprod[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_cumprod_t = torch.sqrt(1-self.alpha_cumprod[t])[:, None, None, None]\n",
    "        xt = sqrt_one_minus_alpha_cumprod_t * x0 + sqrt_alpha_cumprod_t * noise\n",
    "        return xt, noise\n",
    "    \n",
    "    def reverse_diffusion(self, xt, t):\n",
    "        pred_noise = self.model(xt,t)\n",
    "        beta_t = self.beta[t][:, None, None, None]\n",
    "        alpha_t = self.alpha[t][:, None, None, None]\n",
    "        alpha_cumprod_t = self.alpha_cumprod[t][:, None, None, None]\n",
    "        alpha_cumprod_prev_t = self.alpha_cumprod_prev[t][:, None, None, None]\n",
    "        \n",
    "        mean = 1 / torch.sqrt(alpha_t) * (xt-beta_t / torch.sqrt(1-alpha_cumprod_t) * pred_noise)\n",
    "        var = torch.sqrt(1 - alpha_cumprod_prev_t)\n",
    "        \n",
    "        z = torch.randn_like(xt).to(self.device) if t > 0 else 0\n",
    "        xt_prev = mean + var * z\n",
    "        return xt_prev\n",
    "    \n",
    "    def generate(self, batch_size):\n",
    "        xt = torch.randn(batch_size, 3, *self.img_size).to(self.device)\n",
    "        for t in reserved(range(self.timesteps)):\n",
    "            xt = self.reverse_diffusion(xt, torch.full((batch_size,), t, dtype=torch.long).to(self.device))\n",
    "        return xt"
   ],
   "id": "9740ed1392af0a26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model, diffusion, dataloader, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            noisy_imgs, clean_imgs = batch\n",
    "            noisy_imgs, clean_imgs = noisy_imgs.to(device), clean_imgs.to(device)\n",
    "            \n",
    "            t = torch.randint(0, diffusion.timesteps, (noisy_imgs.shape[0],)).to(device)\n",
    "            xt, noise = diffusion.forward_diffusion(clean_imgs, t)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred_noise = model(xt, t)\n",
    "            loss = F.mse_loss(pred_noise, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")"
   ],
   "id": "5c4d36ba32819f56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "img_size = (256,256)\n",
    "timesteps = 1000\n",
    "\n",
    "model = UNet(in_channels=3, out_channels=3).to(device)\n",
    "diffsion = DiffusionModel(model, img_size, timesteps, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_model(model, diffsion, train_loader, optimizer,epochs=10, device=device)\n",
    "\n",
    "torch.save(model.state_dict(), './diffusion_model.pth')"
   ],
   "id": "d78170777972716"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
